{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# since mp3 formats are not \"fully\" supported by librosa\n",
    "# this filters those out warning stating librosa is falling back on another package\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# installing all essential packages to run this file\n",
    "!{sys.executable} -m pip install -r requirements.txt | grep -v 'already satisfied'\n",
    "\n",
    "# for files, audio manipulation, and plotting\n",
    "import csv\n",
    "import random\n",
    "import librosa\n",
    "# import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "from itertools import combinations \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# specific to training the model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Activation, Dropout\n",
    "\n",
    "print(\"Importing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pr> This subsection does NOT need to be run when using the program. It was used to initially load in all the audio files, take 100 mfccs, transform the data, and save the data as numpy arrays for future use. <pr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(num_coefficients):\n",
    "    '''\n",
    "    Loads in all the audio files from (https://toneperfect.lib.msu.edu/). \n",
    "    For each audio file, takes the preferred number of top Mel-frequency cepstrum coefficients (mfccs) and \n",
    "    saves the mcffs in an numpy array and its one hot encoded labels in another numpy array. \n",
    "    Order is maintained between both the data and label arrays. Pads all the audio files so they \n",
    "    are the same duration. Saves all data in a 3D binary numpy array with dimensions being file, \n",
    "    time, coefficients, in that order.\n",
    "    \n",
    "    @param num_coefficients --> the number of top mfccs to take from the tone perfect data \n",
    "    '''\n",
    "    X = list()\n",
    "    y = list()\n",
    "    max_padding = 0\n",
    "\n",
    "    data = Path('.gitignore/tone_perfect_mp3').iterdir()\n",
    "\n",
    "    for item in data:\n",
    "        if item.is_file():\n",
    "            filename = item.name.split(\"_\") #\"{}{}_{}V{}_MP3.mp3\".format(mword, tone, gender, personID)\n",
    "            y.append(int(filename[0][-1]) - 1) #note: labeled 1 --> 4, and now 0 --> 3\n",
    "\n",
    "            #transform all of the data to mfccs (Mel Frequency Cepstral Coefficients)\n",
    "            #https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd\n",
    "            audio, sampling_rate = librosa.load(item)\n",
    "            mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=num_coefficients) \n",
    "            print(mfcc.shape)\n",
    "            \n",
    "            X.append(mfcc)\n",
    "            total_audio_time = len(mfcc[0])\n",
    "            if total_audio_time > max_padding: max_padding = total_audio_time\n",
    "    \n",
    "    #padding each audio file's mcffs with zero so all audio files are considered the same duration \n",
    "    for i in range(len(X)):\n",
    "        X[i] = pad_sequences(X[i], padding='post', maxlen = max_padding)\n",
    "\n",
    "    #transforming from (file, coefficients, time) to (file, time, coefficients)\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    \n",
    "    #save the numpy arrays to significatly cut down on run time \n",
    "    np.save('mcffs_{}.npy'.format(num_coefficients), X)\n",
    "    np.save('labels_{}.npy'.format(num_coefficients), to_categorical(y, num_classes = 4)) \n",
    "    \n",
    "    return None\n",
    "\n",
    "process_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(num_coefficients):\n",
    "    '''\n",
    "    Loads in the data from the pre-processed saved data files that contain the top 100 mfccs for the tonal data\n",
    "    and returns the data with the perferred number of mfccs\n",
    "    \n",
    "    @param num_coefficients --> the number of top mcffs to select for from the data\n",
    "    @return (X, y) --> tuple containing all the data and labels, respectively\n",
    "    '''\n",
    "    X = np.load('mcffs_{}.npy'.format(100))\n",
    "    y = np.load('labels_{}.npy'.format(100))\n",
    "\n",
    "    X = X[:, :, 0:num_coefficients]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y):\n",
    "    '''\n",
    "    Split data into 60% training, 20% testing, and 20% validation\n",
    "    \n",
    "    @param (X, y) --> tuple containing all the data and labels, respectively\n",
    "    @return X_test, y_test, X_train, y_train, X_validation, y_validation --> labeled data split 60/20/20\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = .25)\n",
    "    \n",
    "    return X_test, y_test, X_train, y_train, X_validation, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Websites I referenced:  \n",
    "# https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "# https://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    '''\n",
    "    Generator class used by model to return batches of the data for training and learning paramters\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, labels, timesteps, batch_size, num_tones):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.timesteps = timesteps\n",
    "        self.batch_size = batch_size\n",
    "        self.num_tones = num_tones\n",
    "\n",
    "    def generate_batch(self):\n",
    "        # pre-shuffle all the indices so that every epoch's batch sampling is different\n",
    "        randomized_indices = np.array([*range(0,len(self.data))])\n",
    "        random.shuffle(randomized_indices)\n",
    "        \n",
    "        num_inter = 0\n",
    "        while True:\n",
    "            \n",
    "            start_index = num_inter * self.batch_size\n",
    "            batch_indices = randomized_indices[start_index: self.batch_size + start_index]\n",
    "            data_batch = self.data[batch_indices]\n",
    "            labels_batch = self.labels[batch_indices]\n",
    "            \n",
    "            yield data_batch, labels_batch\n",
    "            \n",
    "            #reshuffle batch indices after going through all the data\n",
    "            num_inter += 1\n",
    "            if len(self.data) <= (num_inter * self.batch_size):\n",
    "                num_inter = 0\n",
    "                random.shuffle(randomized_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_size, timesteps, num_mcffs, dropout_val):\n",
    "    '''\n",
    "    Constructs and returns an untrained Long Short-Term Memory (LSTM) model for the tonal data based on \n",
    "    the paramters provided. Assumes the number of tones is 4.\n",
    "    \n",
    "    @params hidden_size     --> size of the one hidden layer\n",
    "            timesteps       --> number\n",
    "            num_mcffs       --> number of coefficients\n",
    "            dropout_val     --> dropout rate\n",
    "            \n",
    "    @return model --> untrained model\n",
    "    '''\n",
    "    num_tones = 4\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_size, batch_input_shape=(None, timesteps, num_mcffs)))\n",
    "    if dropout_val > 0.0:\n",
    "        model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(num_tones))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tone_data_LSTM(train_data, train_labels, valid_data, valid_labels, num_epochs, dropout_val): \n",
    "    '''\n",
    "    Constructs, trains and returns a Long short-term memory (LSTM) model for the tonal data based on \n",
    "    the paramters provided. Assumes the number of tones is 4, the batch size is 24, and the size of the hidden\n",
    "    layer is 500.\n",
    "    \n",
    "    @params train_data      --> tonal data used for training\n",
    "            train_labels    --> tonal labels that coincide with training data\n",
    "            valid_data      --> tonal data used for validation\n",
    "            valid_labels    --> tonal labels that coincide with validation data\n",
    "            num_epochs      --> number of epochs to run the model on\n",
    "            dropout_val     --> dropout rate\n",
    "            \n",
    "    @return model --> trained model\n",
    "    '''\n",
    "    num_tones = 4\n",
    "    batch_size = 24\n",
    "    hidden_size = 500\n",
    "\n",
    "    timesteps = train_data.shape[1]\n",
    "    num_mcffs = train_data.shape[2]\n",
    "    \n",
    "    X_train_generator = BatchGenerator(train_data, train_labels, timesteps, batch_size, num_tones)                                       \n",
    "    X_validation_generator = BatchGenerator(valid_data, valid_labels, timesteps, batch_size, num_tones)\n",
    "    \n",
    "    model = create_model(hidden_size, timesteps, num_mcffs, dropout_val)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='model_checkpoints/model-{epoch:02d}.hdf5')\n",
    "    model.fit_generator(X_train_generator.generate_batch(), \n",
    "                        len(train_data)//(batch_size*timesteps), \n",
    "                        num_epochs, \n",
    "                        validation_data = X_validation_generator.generate_batch(), \n",
    "                        validation_steps = len(valid_data)//(batch_size*timesteps), \n",
    "                        callbacks=[checkpointer])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Effect of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coefficients = np.arange(5,101,5)\n",
    "hyperparameter_info = dict()\n",
    "\n",
    "for num_coefficients in test_coefficients:\n",
    "    X, y = load_data(num_coefficients)\n",
    "    X_test, y_test, X_train, y_train, X_validation, y_validation = split_data(X, y)\n",
    "    \n",
    "    hyperparameter_info[num_coefficients] = list()\n",
    "    \n",
    "    testing_num_epochs = list(range(50, 301, 50))\n",
    "    testing_dropout_vals = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    for num_epochs in testing_num_epochs:\n",
    "        for dropout_val in testing_dropout_vals:\n",
    "            model = tone_data_LSTM(X_train, y_train, X_validation, y_validation, num_epochs, dropout_val)\n",
    "            prediction = model.predict(X_validation)\n",
    "            accuracy = np.sum(np.argmax(prediction, axis = 1) == y_validation)/len(y_validation)\n",
    "            info = (num_epochs, dropout_val, accuracy)\n",
    "            hyperparameter_info[num_coefficients].append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = csv.writer(open(\"hyperparameter_testing.csv\", \"w\"))\n",
    "for coefficient, val in hyperparameter_info.items():\n",
    "    for (num_epochs, dropout_val, accuracy) in val:\n",
    "        w.writerow([coefficient, num_epochs, dropout_val, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot with accuracy color data point, as a \"4th\" dimension\n",
    "testing_results = np.loadtxt('hyperparameter_testing.csv', delimiter = ',', skiprows= 1)\n",
    "\n",
    "X = testing_results[:, 1] #num_coefficients\n",
    "Y = testing_results[:, 2] #num_epochs\n",
    "Z = testing_results[:, 3] #dropout_prob\n",
    "accuracy = testing_results[:, -1] #acts as the fourth dimension by color mapping\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.view_init(45,60)\n",
    "ax.set_xlabel(\"Mfccs\")\n",
    "ax.set_ylabel(\"Epochs\")\n",
    "ax.set_zlabel(\"Dropout Rate\")\n",
    "img = ax.scatter(X,Y,Z, c = accuracy, cmap = plt.viridis())\n",
    "fig.colorbar(img)\n",
    "plt.show()\n",
    "fig.savefig('3D.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plots\n",
    "testing_results = pd.read_csv(\"hyperparameter_testing.csv\")\n",
    "\n",
    "accuracy = np.asarray(testing_results[\"Accuracy\"])\n",
    "colors = np.empty(accuracy.shape, dtype = 'U25')\n",
    "colors[accuracy <= .97] = '<= .97'\n",
    "colors[(accuracy > .97) & (accuracy <= .98)] = '(.97, .98]'\n",
    "colors[(accuracy > .98) & (accuracy <= .99)] = '(.98, .99]'\n",
    "colors[(accuracy > .99) & (accuracy <= 1)] = '(.99, 1]'\n",
    "\n",
    "testing_results['Accuracy'] = colors\n",
    "colors = sns.diverging_palette(10, 133, n = 4)\n",
    "pair_plot = sns.pairplot(data=testing_results,\n",
    "                  hue=\"Accuracy\", palette=colors,\n",
    "                  size=1.8, aspect=1.8, corner=True)\n",
    "\n",
    "fig = pair_plot.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "fig.savefig('pair_plots.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat maps showing the correlation between all variables\n",
    "testing_results = pd.read_csv(\"hyperparameter_testing.csv\")\n",
    "\n",
    "corr = testing_results.corr()\n",
    "heat_map = sns.heatmap(corr, \n",
    "                 cmap=\"coolwarm\", \n",
    "                 square=True, \n",
    "                 annot=True, \n",
    "                 fmt='.2f', \n",
    "                 annot_kws={\"size\": 14},\n",
    "                 linewidths=.05)\n",
    "\n",
    "fig = heat_map.get_figure()    \n",
    "fig.savefig(\"heat_map.png\", dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-selecting all accuracies from the hyperparameter testing that are above .9935 (5 total)\n",
    "testing_results = pd.read_csv(\"hyperparameter_testing.csv\")\n",
    "top_performers = np.asarray(testing_results.loc[testing_results['Accuracy'] > 0.9935])[:, :-1]\n",
    "\n",
    "#perform 10 fold cross validation an the sets of parameters pre-selected above\n",
    "performances = []\n",
    "\n",
    "for num_coefficients, num_epochs, dropout_rate in top_performers:\n",
    "    X, y = load_data(int(num_coefficients))\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in KFold(10, shuffle=True).split(X):\n",
    "  \n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = .5)\n",
    "        \n",
    "        model = tone_data_LSTM(X_train, y_train, X_validation, y_validation, num_epochs, dropout_rate)\n",
    "        ___, test_accuracy = model.evaluate(X[test_index], y[test_index], verbose=1)\n",
    "        accuracies.append(test_accuracy)\n",
    "        \n",
    "    performances.append(np.mean(accuracies))\n",
    "\n",
    "cross_validation_data = pd.DataFrame(np.hstack((top_performers,np.asarray(performances).reshape(5,1))),\n",
    "                                    columns = [\"Num Coefficients\",\"Num Epochs\", \"Dropout Probability\", \"Accuracy\"])\n",
    "cross_validation_data.to_csv('cross_validation_results.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Saving The Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the model that had the highest accuracy during 10-fold cross validation\n",
    "validation_results = pd.read_csv(\"cross_validation_results.csv\")\n",
    "top_accuracy = validation_results['Accuracy'].max()\n",
    "top_performer = validation_results[validation_results['Accuracy'] == top_accuracy]\n",
    "\n",
    "# getting the optimal hyperparameters\n",
    "num_coefficients, num_epochs, dropout_rate = np.asarray(top_performer)[:, 1:-1][0]\n",
    "\n",
    "# data\n",
    "X, y = load_data(int(num_coefficients))\n",
    "X_test, y_test, X_train, y_train, X_validation, y_validation = split_data(X,y)\n",
    "\n",
    "# training\n",
    "model = tone_data_LSTM(X_train, y_train, X_validation, y_validation, num_epochs, dropout_rate)\n",
    "\n",
    "# saving model\n",
    "# Found from: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model_spec.txt', 'w')\n",
    "file.write(\"No. of coefficients: {}\".format(num_coefficients)) \n",
    "file.write(\"\\nNo. of epochs: {}\".format(num_epochs)) \n",
    "file.write(\"\\nNo. of dropout_rate: {}\".format(dropout_rate)) \n",
    "file.write(\"\\nModel Accuracy from 10-Fold cross validation: {}\".format(top_accuracy)) \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pr> User's should interact with the notebook through this section. All imports that are needed are included under the import section right below. Once that is run, the use can load in the model that was previously found to have the highest accuracy. From there, the user can create a folder to hold all their recordings in case they want to go back to them later and listen to themselves learn over time. The user is then able to record their voice, play it back, and and get real time auditory feedback from the model predicting their tone. They may do this as many times as they wish! At the end, they can delete all their recordings. In order to run a cell, a user should click on the cell, then press the run button up above. Have fun and good luck in learning Mandarin tones! <pr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since mp3 formats are not \"fully\" supported by librosa\n",
    "# this filters those out warning stating librosa is falling back on another package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"Importing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found from: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "json_file = open('model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(\"model.h5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "file = open('model_spec.txt', 'r') \n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folder to hold user audio\n",
    "<p> Run the following cell to remove previous user audio and make the folder to hold all user input audio. <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing folder to hold user audio\n",
    "user_audio_directory = (\"user_audio\")\n",
    "try:\n",
    "    shutil.rmtree(user_audio_directory)\n",
    "    os.makedirs(user_audio_directory)\n",
    "    print(\"Removed data and recreated folder\", user_audio_directory)\n",
    "except(FileNotFoundError):\n",
    "    os.makedirs(user_audio_directory)\n",
    "    print(\"Created folder {}\".format(user_audio_directory)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record user's voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pr> Users are encouraged to only record clear monosyllabic words in order to practice tonalities.<pr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 44100\n",
    "duration = 1.4 #aligns with the max time for the data recordings [sec]\n",
    "\n",
    "print(\"Recording in...\")\n",
    "pause = 3\n",
    "\n",
    "while pause > 0:\n",
    "    print(\"{}\".format(pause))\n",
    "    pause -= 1\n",
    "    time.sleep(1)\n",
    "print(\"RECORDING\")\n",
    "\n",
    "recording = sd.rec(int(duration * frame_rate), samplerate = frame_rate, channels = 1)\n",
    "recording_num = len(os.listdir('user_audio/')) + 1\n",
    "recording_name = 'user_audio/sample_audio_{}.mp3'.format(recording_num)\n",
    "sd.wait()\n",
    "write(recording_name, frame_rate, recording)\n",
    "\n",
    "print(\"\\nFinished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot, playback, and prediction of user recording \n",
    "<pr> User can play back and see a visualization of their latest recording as many times as they like. <pr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    audio, sampling_rate = librosa.load(recording_name, duration=1.5)\n",
    "    \n",
    "    ###plot##\n",
    "    timeline = np.arange(0, len(audio))/sampling_rate\n",
    "    fig = plt.figure()\n",
    "    plt.plot(timeline, audio)\n",
    "    plt.xlabel(\"Time (s)\", fontsize=18)\n",
    "    plt.ylabel(\"Amplitude\", fontsize=16)\n",
    "               \n",
    "    ###playback##\n",
    "    sd.play(audio, sampling_rate)\n",
    "    sd.wait()\n",
    "    sd.stop()\n",
    "    \n",
    "    ##predict##\n",
    "    #transform data\n",
    "    user_mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=80) \n",
    "    padded_user_mfcc = np.asarray([pad_sequences(user_mfcc, padding='post', maxlen = 60)])\n",
    "    user_audio_test_data = np.transpose(padded_user_mfcc, (0, 2, 1))\n",
    "    \n",
    "    #predict tone using model\n",
    "    prediction_accuracies = model.predict(user_audio_test_data) \n",
    "    predicted_tone = np.where((prediction_accuracies > 0.5).astype(\"int32\")[0] == 1)[0][0]\n",
    "    \n",
    "    #report prediction\n",
    "    tone = predicted_tone+ 1 #accounting for data labels being indexed at zero\n",
    "    closeness = \"{:.2%}\".format(prediction_accuracies[0][predicted_tone])\n",
    "\n",
    "    print(\"The model predicts you are speaking in tone {} with an accuracy of {}.\".format(tone, closeness))\n",
    "    \n",
    "except(FileNotFoundError):\n",
    "    print(\"Please go back and run the previous two cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all user recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_audio_directory = (\"user_audio\")\n",
    "try:\n",
    "    shutil.rmtree(user_audio_directory)\n",
    "    print(\"Successfully removed user data\")\n",
    "except(FileNotFoundError):\n",
    "    print(\"User data already deleted\".format(user_audio_directory)) "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
